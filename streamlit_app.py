import streamlit as st
import math
import pandas as pd
import matplotlib.pyplot as plt
from googleapiclient.discovery import build
from datetime import datetime, timedelta, timezone
from collections import Counter
import re
import time
import requests
import json
import numpy as np

# Streamlit ì•± ì„¤ì •
st.set_page_config(
    page_title="í†µí•© í‚¤ì›Œë“œ ë¶„ì„ê¸°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • (Streamlit í™˜ê²½)
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# --- 1. YouTube ë¶„ì„ í´ë˜ìŠ¤ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
class YouTubeSpreadAnalyzer:
    def __init__(self, api_key):
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.api_key = api_key
    
    def search_videos_by_keyword(self, keyword, max_results=100, days_back=30):
        """í‚¤ì›Œë“œë¡œ ë¹„ë””ì˜¤ ê²€ìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜ ë° ë°°ì¹˜ ì²˜ë¦¬)"""
        published_after = (datetime.now(timezone.utc) - timedelta(days=days_back)).isoformat()
        video_ids = []
        next_page_token = None
        remaining_results = max_results
        
        while remaining_results > 0:
            current_max = min(50, remaining_results)
            try:
                search_response = self.youtube.search().list(
                    q=keyword,
                    part="id,snippet",
                    maxResults=current_max,
                    pageToken=next_page_token,
                    order="viewCount",
                    publishedAfter=published_after,
                    type="video",
                    relevanceLanguage="ko"
                ).execute()
            except Exception as e:
                st.error(f"YouTube API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                break

            for item in search_response['items']:
                if item['id']['kind'] == 'youtube#video':
                    video_ids.append(item['id']['videoId'])
            
            next_page_token = search_response.get('nextPageToken')
            if not next_page_token:
                break
            
            remaining_results -= current_max
            time.sleep(0.5)
        
        actual_count = len(video_ids)
        if actual_count == 0:
            return []
        
        videos = []
        batch_size = 50
        for i in range(0, actual_count, batch_size):
            batch_ids = video_ids[i:i + batch_size]
            try:
                video_response = self.youtube.videos().list(
                    part="statistics,snippet",
                    id=",".join(batch_ids)
                ).execute()
            except Exception as e:
                st.error(f"YouTube API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                break

            for item in video_response['items']:
                view_count = int(item['statistics'].get('viewCount', 0))
                like_count = int(item['statistics'].get('likeCount', 0))
                comment_count = int(item['statistics'].get('commentCount', 0))
                
                videos.append({
                    'id': item['id'],
                    'title': item['snippet']['title'],
                    'channelTitle': item['snippet']['channelTitle'],
                    'publishedAt': item['snippet']['publishedAt'],
                    'viewCount': view_count,
                    'likeCount': like_count,
                    'commentCount': comment_count
                })
            
            time.sleep(0.5)
        
        return videos
    
    def calculate_spread_coefficient(self, videos):
        if not videos:
            return 0.0, 0.0, 0, 0.0
        
        total_views = 0
        total_weighted = 0
        video_count = len(videos)
        
        for video in videos:
            view_count = video['viewCount']
            like_count = video['likeCount']
            comment_count = video['commentCount']
            
            engagement = 0
            if view_count > 0:
                engagement = min(0.45, (like_count + comment_count) / view_count)
            
            weighted_views = view_count * (1 + engagement)
            total_weighted += weighted_views
            total_views += view_count
        
        avg_views = total_views / video_count
        avg_weighted = total_weighted / video_count
        
        if avg_weighted <= 1000:
            spread_coefficient = 0
        elif avg_weighted >= 5_000_000:
            spread_coefficient = 10.0
        else:
            spread_coefficient = (math.log10(avg_weighted) - 3) * (10 / (math.log10(5_000_000) - 3))
        
        return spread_coefficient, avg_weighted, total_views, avg_views
    
    def extract_common_keywords(self, titles, top_n=10):
        if not titles:
            return []
        
        stop_words = ["ì˜ìƒ", "ì¶”ì²œ", "ë¹„ë””ì˜¤", "Youtube", "YouTube", "ë³´ê¸°", "ìµœì‹ ", "ì¸ê¸°", "ê¸‰ìƒìŠ¹", "ê³µê°œ", "í’€ì˜ìƒ", "í’€ë²„ì „", "ê³µì‹"]
        words = []
        for title in titles:
            clean_title = re.sub(r'[^\w\s#]', '', title)
            hashtags = re.findall(r'#(\w+)', clean_title)
            words.extend(hashtags)
            korean_words = re.findall(r'[\w#]*[ê°€-í£]{2,}[\w#]*', clean_title)
            words.extend(korean_words)
        
        filtered_words = [word for word in words if word not in stop_words and len(word) > 1]
        word_counts = Counter(filtered_words)
        
        return [f"#{word}" for word, _ in word_counts.most_common(top_n)]
    
    def analyze_keyword_spread(self, keyword, days_back=30, max_results=100):
        videos = self.search_videos_by_keyword(keyword, max_results, days_back)
        
        if not videos:
            return {"error": "ë™ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}, None
        
        spread_coeff, avg_weighted, total_views, avg_views = self.calculate_spread_coefficient(videos)
        
        top_videos = sorted(videos, key=lambda x: x['viewCount'], reverse=True)[:10]
        top_titles = [video['title'] for video in top_videos]
        common_keywords = self.extract_common_keywords(top_titles)
        
        result = {
            "keyword": keyword,
            "total_videos": len(videos),
            "total_views": total_views,
            "avg_views": avg_views,
            "avg_weighted_views": avg_weighted,
            "spread_coefficient": spread_coeff,
            "top_videos": top_videos,
            "common_keywords": common_keywords[:5],
            "days_back": days_back
        }
        
        return result, videos

# --- 2. Naver ë¶„ì„ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def get_naver_search_index(keywords_dict, start_date, end_date):
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": st.secrets["NAVER_CLIENT_ID"],
        "X-Naver-Client-Secret": st.secrets["NAVER_CLIENT_SECRET"],
        "Content-Type": "application/json"
    }
    
    keyword_groups = []
    for group_name, keywords in keywords_dict.items():
        keyword_groups.append({
            "groupName": group_name,
            "keywords": keywords
        })
    
    body = {
        "startDate": start_date,
        "endDate": end_date,
        "timeUnit": "date",
        "keywordGroups": keyword_groups
    }
    
    res = requests.post(url, json=body, headers=headers)
    res.raise_for_status()
    return res.json()

def calculate_absolute_index(main_data, ref_data_list):
    ref_max = 0
    for ref_df in ref_data_list:
        ref_max = max(ref_max, ref_df['ratio'].max())
    
    return (main_data['ratio'] / ref_max) * 100

def calculate_bti(naver_abs_index):
    return naver_abs_index / 100.0

# --- Streamlit UI ë° ë©”ì¸ ë¡œì§ ---
st.title("í†µí•© í‚¤ì›Œë“œ í™•ì‚° ë¶„ì„ê¸°")
st.markdown("ìœ íŠœë¸Œì™€ ë„¤ì´ë²„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í‚¤ì›Œë“œì˜ í™•ì‚°ë ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("ğŸ”‘ í‚¤ì›Œë“œ ë° ì„¤ì •")
    keyword = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œ", "ì°½ê³ í˜• ì•½êµ­")
    days_back = st.slider("ë¶„ì„ ê¸°ê°„ (ì¼)", 7, 365, 30)
    max_results = st.slider("YouTube ë¶„ì„ ë™ì˜ìƒ ìˆ˜", 10, 200, 100)
    
    # ê¸°ì¤€ í‚¤ì›Œë“œ (ë„¤ì´ë²„ BTIìš©)
    st.subheader("ë„¤ì´ë²„ BTI ê¸°ì¤€ í‚¤ì›Œë“œ")
    ref_keywords_str = st.text_input("ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥", "ë‰´ìŠ¤,ë‚ ì”¨,ìœ íŠœë¸Œ")
    reference_keywords = [kw.strip() for kw in ref_keywords_str.split(',') if kw.strip()]
    
    run_button = st.button("ğŸš€ ë¶„ì„ ì‹œì‘")

if run_button and keyword:
    try:
        # íƒ­ UI
        tab1, tab2 = st.tabs(["ğŸ“Š ìœ íŠœë¸Œ í™•ì‚° ë¶„ì„", "ğŸ“ˆ ë„¤ì´ë²„ BTI ë¶„ì„"])

        with tab1:
            st.header("ğŸ“Š ìœ íŠœë¸Œ í™•ì‚° ë¶„ì„ ê²°ê³¼")
            with st.spinner("YouTube ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                youtube_analyzer = YouTubeSpreadAnalyzer(st.secrets["YOUTUBE_API_KEY"])
                result, videos = youtube_analyzer.analyze_keyword_spread(
                    keyword,
                    days_back=days_back,
                    max_results=max_results
                )
            
            if "error" in result:
                st.error(f"ì˜¤ë¥˜ ë°œìƒ: {result['error']}")
            else:
                st.subheader(f"'{result['keyword']}' í‚¤ì›Œë“œ í™•ì‚° ë¶„ì„ (ìµœê·¼ {result['days_back']}ì¼ ê¸°ì¤€)")
                st.metric("í™•ì‚° ê³„ìˆ˜ (SC)", f"{result['spread_coefficient']:.2f} / 10.00")
                
                st.info(f"**ì´ ì¡°íšŒìˆ˜**: {result['total_views']:,}íšŒ | **í‰ê·  ì¡°íšŒìˆ˜**: {result['avg_views']:,.1f}íšŒ | **í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜**: {result['avg_weighted_views']:,.1f}íšŒ")
                
                # SC í•´ì„
                sc = result['spread_coefficient']
                sc_guide = ""
                if sc < 2.0: sc_guide = "ë¯¸ë¯¸í•œ ì˜í–¥ (ì—¬ë¡  ë³€ë™ ì—†ìŒ)"
                elif sc < 4.0: sc_guide = "ì£¼ëª© ìš”ë§ (ì´ìŠˆí™” ê°€ëŠ¥ì„±)"
                elif sc < 6.0: sc_guide = "ìœ ì˜ë¯¸í•œ ì˜í–¥ (ê³µë¡ í™” ì‹œì‘)"
                elif sc < 8.0: sc_guide = "ì‹¬ê°í•œ ì˜í–¥ (ì—¬ë¡  ì•…í™”)"
                elif sc < 10.0: sc_guide = "ìœ„ê¸° ìˆ˜ì¤€ (êµ­ì • ìš´ì˜ ì˜í–¥)"
                else: sc_guide = "ìµœê³  ìœ„ê¸° ìˆ˜ì¤€ (ì„ ê±° íŒë„ ë³€ê²½ ê°€ëŠ¥ì„±)"
                st.markdown(f"**í•´ì„**: {sc_guide}")

                st.markdown(f"**ì¶”ì²œ í•´ì‹œíƒœê·¸**: `{'`, `'.join(result['common_keywords'])}`")
                
                st.subheader("ìƒìœ„ ë™ì˜ìƒ ëª©ë¡")
                top_videos_df = pd.DataFrame(result['top_videos'])
                top_videos_df['publishedAt'] = pd.to_datetime(top_videos_df['publishedAt']).dt.strftime('%Y-%m-%d')
                top_videos_df = top_videos_df[['channelTitle', 'title', 'viewCount', 'likeCount', 'commentCount', 'publishedAt']]
                st.dataframe(top_videos_df, use_container_width=True)
                
                st.subheader("ì£¼ìš” ì‹œê°í™”")
                # ì‹œê°í™” ë¶€ë¶„ (Streamlitì— ë§ê²Œ ìˆ˜ì •)
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
                
                # 1. ì¡°íšŒìˆ˜ ë¶„í¬
                df_videos = pd.DataFrame(videos).sort_values('viewCount', ascending=False)
                ax1.bar(range(len(df_videos)), df_videos['viewCount'], color='skyblue')
                ax1.set_title('ë¹„ë””ì˜¤ë³„ ì¡°íšŒìˆ˜ ë¶„í¬')
                ax1.set_xlabel('ë¹„ë””ì˜¤ ìˆœì„œ (ì¡°íšŒìˆ˜ ê¸°ì¤€)')
                ax1.set_ylabel('ì¡°íšŒìˆ˜')
                
                # 2. í™•ì‚° ì§€í‘œ ë¹„êµ
                metrics = ['ì´ ì¡°íšŒìˆ˜', 'í‰ê·  ì¡°íšŒìˆ˜', 'í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜']
                values = [
                    result['total_views'],
                    result['avg_views'],
                    result['avg_weighted_views']
                ]
                colors = ['blue', 'green', 'orange']
                ax2.bar(metrics, values, color=colors)
                ax2.set_title('í™•ì‚° ì§€í‘œ ë¹„êµ')
                ax2.set_ylabel('ê°’')
                ax2.ticklabel_format(style='plain', axis='y')
                
                plt.tight_layout()
                st.pyplot(fig)

        with tab2:
            st.header("ğŸ“ˆ ë„¤ì´ë²„ BTI ë¶„ì„ ê²°ê³¼")
            with st.spinner("Naver ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                end_date = datetime.now().strftime("%Y-%m-%d")
                start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
                
                keywords_dict = {"main": [keyword]}
                for i, ref_kw in enumerate(reference_keywords):
                    keywords_dict[f"ref_{i}"] = [ref_kw]
                
                try:
                    naver_raw = get_naver_search_index(keywords_dict, start_date, end_date)
                    
                    results = {}
                    for res in naver_raw['results']:
                        group_name = res['title']
                        df = pd.DataFrame(res['data'])
                        df['date'] = pd.to_datetime(df['period'])
                        results[group_name] = df
                    
                    main_df = results['main']
                    ref_dfs = [df for key, df in results.items() if key.startswith('ref_')]
                    
                    main_df['abs_index'] = calculate_absolute_index(main_df, ref_dfs)
                    main_df['bti'] = calculate_bti(main_df['abs_index'])
                    
                    st.subheader(f"'{keyword}' í‚¤ì›Œë“œ BTI ë¶„ì„ (ìµœê·¼ {days_back}ì¼ ê¸°ì¤€)")
                    st.metric("ìµœê·¼ 30ì¼ í‰ê·  BTI", f"{main_df['bti'].tail(30).mean():.2f}")
                    st.markdown("ìµœê·¼ 7ì¼ BTI ì¶”ì´:")
                    st.dataframe(main_df[['date', 'bti']].tail(7).set_index('date'), use_container_width=True)

                    st.subheader("BTI ì§€ìˆ˜ ì¶”ì´ ê·¸ë˜í”„")
                    
                    fig, ax = plt.subplots(figsize=(12, 6))
                    ax.plot(main_df['date'], main_df['bti'], 'b-', linewidth=2, label='BTI ì§€ìˆ˜')
                    
                    main_df['30d_ma'] = main_df['bti'].rolling(window=30).mean()
                    ax.plot(main_df['date'], main_df['30d_ma'], 'r--', linewidth=2, label='30ì¼ ì´ë™í‰ê· ')
                    
                    ax.set_title(f'BTI Index Trend: {keyword} (Naver)')
                    ax.set_xlabel('ë‚ ì§œ')
                    ax.set_ylabel('BTI ì§€ìˆ˜')
                    ax.grid(True, linestyle='--', alpha=0.7)
                    ax.legend()
                    st.pyplot(fig)
                
                except requests.exceptions.HTTPError as err:
                    if err.response.status_code == 401:
                        st.error("ë„¤ì´ë²„ API ì¸ì¦ ì˜¤ë¥˜: Client ID ë˜ëŠ” Client Secretì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    else:
                        st.error(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {err}")
                except Exception as e:
                    st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    except Exception as e:
        st.error(f"ë¶„ì„ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        st.warning("API í‚¤ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")