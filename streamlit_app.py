import streamlit as st
import math
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from googleapiclient.discovery import build
from datetime import datetime, timedelta, timezone
from collections import Counter
import re
import time
import requests
import json
import numpy as np
from wordcloud import WordCloud


# Streamlit ì•± ì„¤ì •
st.set_page_config(
    page_title="í†µí•© í‚¤ì›Œë“œ ë¶„ì„ê¸°",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- í°íŠ¸ ë¡œë“œ (í”„ë¡œì íŠ¸ í´ë”ì— 'font/malgun.ttf'ê°€ ìˆì–´ì•¼ í•¨) ---
try:
    font_path = 'font/malgun.ttf'
    fm.fontManager.addfont(font_path)
    font_name = fm.FontProperties(fname=font_path).get_name()
    plt.rcParams['font.family'] = font_name
    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    st.warning(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
    st.info("í”„ë¡œì íŠ¸ í´ë”ì— 'font' í´ë”ë¥¼ ë§Œë“¤ê³  'malgun.ttf' íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")


# --- 1. YouTube ë¶„ì„ í´ë˜ìŠ¤ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
class YouTubeSpreadAnalyzer:
    def __init__(self, api_key):
        self.youtube = build('youtube', 'v3', developerKey=api_key)
        self.api_key = api_key
    
    def search_videos_by_keyword(self, keyword, max_results=100, days_back=30):
        """í‚¤ì›Œë“œë¡œ ë¹„ë””ì˜¤ ê²€ìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜ ë° ë°°ì¹˜ ì²˜ë¦¬)"""
        published_after = (datetime.now(timezone.utc) - timedelta(days=days_back)).isoformat()
        video_ids = []
        next_page_token = None
        remaining_results = max_results
        
        while remaining_results > 0:
            current_max = min(50, remaining_results)
            try:
                search_response = self.youtube.search().list(
                    q=keyword,
                    part="id,snippet",
                    maxResults=current_max,
                    pageToken=next_page_token,
                    order="viewCount",
                    publishedAfter=published_after,
                    type="video",
                    relevanceLanguage="ko"
                ).execute()
            except Exception as e:
                st.error(f"YouTube API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                break

            for item in search_response['items']:
                if item['id']['kind'] == 'youtube#video':
                    video_ids.append(item['id']['videoId'])
            
            next_page_token = search_response.get('nextPageToken')
            if not next_page_token:
                break
            
            remaining_results -= current_max
            time.sleep(0.5)
        
        actual_count = len(video_ids)
        if actual_count == 0:
            return []
        
        videos = []
        batch_size = 50
        for i in range(0, actual_count, batch_size):
            batch_ids = video_ids[i:i + batch_size]
            try:
                video_response = self.youtube.videos().list(
                    part="statistics,snippet",
                    id=",".join(batch_ids)
                ).execute()
            except Exception as e:
                st.error(f"YouTube API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                break

            for item in video_response['items']:
                view_count = int(item['statistics'].get('viewCount', 0))
                like_count = int(item['statistics'].get('likeCount', 0))
                comment_count = int(item['statistics'].get('commentCount', 0))
                
                videos.append({
                    'id': item['id'],
                    'title': item['snippet']['title'],
                    'channelTitle': item['snippet']['channelTitle'],
                    'publishedAt': item['snippet']['publishedAt'],
                    'viewCount': view_count,
                    'likeCount': like_count,
                    'commentCount': comment_count
                })
            
            time.sleep(0.5)
        
        return videos
    
    def calculate_spread_coefficient(self, videos):
        if not videos:
            return 0.0, 0.0, 0, 0.0
        
        total_views = 0
        total_weighted = 0
        video_count = len(videos)
        
        for video in videos:
            view_count = video['viewCount']
            like_count = video['likeCount']
            comment_count = video['commentCount']
            
            engagement = 0
            if view_count > 0:
                engagement = min(0.45, (like_count + comment_count) / view_count)
            
            weighted_views = view_count * (1 + engagement)
            total_weighted += weighted_views
            total_views += view_count
        
        avg_views = total_views / video_count
        avg_weighted = total_weighted / video_count
        
        if avg_weighted <= 1000:
            spread_coefficient = 0
        elif avg_weighted >= 5_000_000:
            spread_coefficient = 10.0
        else:
            spread_coefficient = (math.log10(avg_weighted) - 3) * (10 / (math.log10(5_000_000) - 3))
        
        return spread_coefficient, avg_weighted, total_views, avg_views
    
    def extract_common_keywords(self, titles, top_n=10):
        if not titles:
            return []
        
        stop_words = ["ì˜ìƒ", "ì¶”ì²œ", "ë¹„ë””ì˜¤", "Youtube", "YouTube", "ë³´ê¸°", "ìµœì‹ ", "ì¸ê¸°", "ê¸‰ìƒìŠ¹", "ê³µê°œ", "í’€ì˜ìƒ", "í’€ë²„ì „", "ê³µì‹"]
        words = []
        for title in titles:
            clean_title = re.sub(r'[^\w\s#]', '', title)
            hashtags = re.findall(r'#(\w+)', clean_title)
            words.extend(hashtags)
            korean_words = re.findall(r'[\w#]*[ê°€-í£]{2,}[\w#]*', clean_title)
            words.extend(korean_words)
        
        filtered_words = [word for word in words if word not in stop_words and len(word) > 1]
        word_counts = Counter(filtered_words)
        
        return [f"#{word}" for word, _ in word_counts.most_common(top_n)]
    
    def analyze_keyword_spread(self, keyword, days_back=30, max_results=100):
        videos = self.search_videos_by_keyword(keyword, max_results, days_back)
        
        if not videos:
            return {"error": "ë™ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}, None
        
        spread_coeff, avg_weighted, total_views, avg_views = self.calculate_spread_coefficient(videos)
        
        top_videos = sorted(videos, key=lambda x: x['viewCount'], reverse=True)[:10]
        top_titles = [video['title'] for video in top_videos]
        common_keywords = self.extract_common_keywords(top_titles)
        
        result = {
            "keyword": keyword,
            "total_videos": len(videos),
            "total_views": total_views,
            "avg_views": avg_views,
            "avg_weighted_views": avg_weighted,
            "spread_coefficient": spread_coeff,
            "top_videos": top_videos,
            "common_keywords": common_keywords[:5],
            "days_back": days_back
        }
        
        return result, videos

# --- 2. Naver ë¶„ì„ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼) ---
def get_naver_search_index(keywords_dict, start_date, end_date):
    url = "https://openapi.naver.com/v1/datalab/search"
    headers = {
        "X-Naver-Client-Id": st.secrets["NAVER_CLIENT_ID"],
        "X-Naver-Client-Secret": st.secrets["NAVER_CLIENT_SECRET"],
        "Content-Type": "application/json"
    }
    
    keyword_groups = []
    for group_name, keywords in keywords_dict.items():
        keyword_groups.append({
            "groupName": group_name,
            "keywords": keywords
        })
    
    body = {
        "startDate": start_date,
        "endDate": end_date,
        "timeUnit": "date",
        "keywordGroups": keyword_groups
    }
    
    res = requests.post(url, json=body, headers=headers)
    res.raise_for_status()
    return res.json()

def calculate_absolute_index(main_data, ref_data_list):
    ref_max = 0
    for ref_df in ref_data_list:
        ref_max = max(ref_max, ref_df['ratio'].max())
    
    return (main_data['ratio'] / ref_max) * 100

def calculate_bti(naver_abs_index):
    return naver_abs_index

def calculate_combined_index(sc_value, bti_df):
    avg_bti = bti_df['bti'].tail(len(bti_df)).mean()
    combined_index = (sc_value * 10.0 + avg_bti) / 2.0
    return combined_index


# --- ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜ ---
def create_wordcloud(text, font_path):
    wordcloud = WordCloud(
        font_path=font_path,
        background_color="white",
        width=800,
        height=400,
        max_words=50
    ).generate(text)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig)

# --- ì‚¬ìš©ì ì¸ì¦ ---
st.header("ğŸ”‘ í†µí•© í‚¤ì›Œë“œ ë¶„ì„ê¸° ë¡œê·¸ì¸")
PASSWORD = st.secrets["APP_PASSWORD"]
if 'logged_in' not in st.session_state:
    st.session_state.logged_in = False

if st.session_state.logged_in:
    # --- Streamlit UI ë° ë©”ì¸ ë¡œì§ ---
    st.title("í†µí•© í‚¤ì›Œë“œ í™•ì‚° ë¶„ì„ê¸°")
    st.markdown("ìœ íŠœë¸Œì™€ ë„¤ì´ë²„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í‚¤ì›Œë“œì˜ í™•ì‚°ë ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("ğŸ”‘ í‚¤ì›Œë“œ ë° ì„¤ì •")
        keyword = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œ", placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
        days_back = st.slider("ë¶„ì„ ê¸°ê°„ (ì¼)", 7, 365, 30)
        max_results = st.slider("YouTube ë¶„ì„ ë™ì˜ìƒ ìˆ˜", 10, 200, 100)
        
        st.subheader("ë„¤ì´ë²„ BTI ê¸°ì¤€ í‚¤ì›Œë“œ")
        ref_keywords_str = st.text_input("ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥", "ë‰´ìŠ¤,ë‚ ì”¨")
        reference_keywords = [kw.strip() for kw in ref_keywords_str.split(',') if kw.strip()]
        
        run_button = st.button("ğŸš€ ë¶„ì„ ì‹œì‘")

    if run_button and keyword:
        try:
            tab1, tab2 = st.tabs(["ğŸ“Š ìœ íŠœë¸Œ í™•ì‚° ë¶„ì„", "ğŸ“ˆ ë„¤ì´ë²„ BTI ë¶„ì„"])

            with tab1:
                st.header("ğŸ“Š ìœ íŠœë¸Œ í™•ì‚° ë¶„ì„ ê²°ê³¼")
                with st.spinner("YouTube ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                    youtube_analyzer = YouTubeSpreadAnalyzer(st.secrets["YOUTUBE_API_KEY"])
                    result, videos = youtube_analyzer.analyze_keyword_spread(
                        keyword,
                        days_back=days_back,
                        max_results=max_results
                    )
                
                if "error" in result:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {result['error']}")
                else:
                    st.subheader(f"'{result['keyword']}' í‚¤ì›Œë“œ í™•ì‚° ë¶„ì„ (ìµœê·¼ {result['days_back']}ì¼ ê¸°ì¤€)")
                    st.metric("í™•ì‚° ê³„ìˆ˜ (SC)", f"{result['spread_coefficient']:.2f} / 10.00")
                    
                    with st.expander("í™•ì‚° ê³„ìˆ˜(SC) ê³µì‹ ë³´ê¸°"):
                        st.markdown(r"""
                            $ SC_{coeff} = \frac{log_{10}(WAV) - 3}{log_{10}(5,000,000) - 3} \times 10 $
                            <br>
                            **WAV**: ê°€ì¤‘ í‰ê·  ì¡°íšŒìˆ˜ (Weighted Average Views) = ì¼ë°˜ ì¡°íšŒìˆ˜ Ã— (1 + ì°¸ì—¬ë„)
                            <br>
                            **ì°¸ì—¬ë„**: (ì¢‹ì•„ìš” + ëŒ“ê¸€) / ì¡°íšŒìˆ˜
                            """, unsafe_allow_html=True)
                    
                    st.info(f"**ì´ ì¡°íšŒìˆ˜**: {result['total_views']:,}íšŒ | **í‰ê·  ì¡°íšŒìˆ˜**: {result['avg_views']:,.1f}íšŒ | **í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜**: {result['avg_weighted_views']:,.1f}íšŒ")
                    
                    # ìƒˆë¡œìš´ ì§€í‘œ: í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜ / í‰ê·  ì¡°íšŒìˆ˜ ë¹„ìœ¨
                    if result['avg_views'] > 0:
                        engagement_ratio = result['avg_weighted_views'] / result['avg_views']
                        st.metric("ì°¸ì—¬ë„ ì˜í–¥ë ¥ (ê°€ì¤‘/ì¼ë°˜ ì¡°íšŒìˆ˜)", f"{engagement_ratio:.2f}")
                    
                    sc = result['spread_coefficient']
                    sc_guide = ""
                    if sc < 2.0: sc_guide = "ë¯¸ë¯¸í•œ ì˜í–¥"
                    elif sc < 4.0: sc_guide = "ì£¼ëª© ìš”ë§"
                    elif sc < 6.0: sc_guide = "ìœ ì˜ë¯¸í•œ ì˜í–¥"
                    elif sc < 8.0: sc_guide = "ì‹¬ê°í•œ ì˜í–¥"
                    elif sc < 10.0: sc_guide = "ìœ„ê¸° ìˆ˜ì¤€"
                    else: sc_guide = "ìµœê³  ìœ„ê¸° ìˆ˜ì¤€"
                    st.markdown(f"**í•´ì„**: {sc_guide}")

                    st.markdown(f"**ì¶”ì²œ í•´ì‹œíƒœê·¸**: `{'`, `'.join(result['common_keywords'])}`")
                    
                    st.subheader("ìƒìœ„ ë™ì˜ìƒ ëª©ë¡")
                    top_videos_df = pd.DataFrame(result['top_videos'])
                    top_videos_df['publishedAt'] = pd.to_datetime(top_videos_df['publishedAt']).dt.strftime('%Y-%m-%d')
                    top_videos_df = top_videos_df[['channelTitle', 'title', 'viewCount', 'likeCount', 'commentCount', 'publishedAt']]
                    st.dataframe(top_videos_df, use_container_width=True)
                    
                    st.subheader("ì£¼ìš” ì‹œê°í™”")
                    fig, ax = plt.subplots(figsize=(16, 6))
                    metrics = ['ì´ ì¡°íšŒìˆ˜', 'í‰ê·  ì¡°íšŒìˆ˜', 'í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜']
                    values = [
                        result['total_views'],
                        result['avg_views'],
                        result['avg_weighted_views']
                    ]
                    colors = ['blue', 'green', 'orange']
                    ax.bar(metrics, values, color=colors)
                    ax.set_title('í™•ì‚° ì§€í‘œ ë¹„êµ')
                    ax.set_ylabel('ê°’')
                    ax.ticklabel_format(style='plain', axis='y')
                    plt.tight_layout()
                    st.pyplot(fig)
                    
                    # ì›Œë“œí´ë¼ìš°ë“œ
                    all_titles_text = " ".join([video['title'] for video in videos])
                    st.subheader("ğŸ’¬ ì˜ìƒ ì œëª© ì›Œë“œ í´ë¼ìš°ë“œ")
                    if all_titles_text:
                        create_wordcloud(all_titles_text, font_path)
                    else:
                        st.info("ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤.")


            with tab2:
                st.header("ğŸ“ˆ ë„¤ì´ë²„ BTI ë¶„ì„ ê²°ê³¼")
                with st.spinner("Naver ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                    end_date = datetime.now().strftime("%Y-%m-%d")
                    start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
                    
                    keywords_dict = {"main": [keyword]}
                    for i, ref_kw in enumerate(reference_keywords):
                        keywords_dict[f"ref_{i}"] = [ref_kw]
                    
                    try:
                        naver_raw = get_naver_search_index(keywords_dict, start_date, end_date)
                        
                        results = {}
                        for res in naver_raw['results']:
                            group_name = res['title']
                            df = pd.DataFrame(res['data'])
                            df['date'] = pd.to_datetime(df['period'])
                            results[group_name] = df
                        
                        main_df = results['main']
                        ref_dfs = [df for key, df in results.items() if key.startswith('ref_')]
                        
                        main_df['abs_index'] = calculate_absolute_index(main_df, ref_dfs)
                        main_df['bti'] = calculate_bti(main_df['abs_index'])
                        
                        st.subheader(f"'{keyword}' í‚¤ì›Œë“œ BTI ë¶„ì„ (ìµœê·¼ {days_back}ì¼ ê¸°ì¤€)")
                        st.metric(f"ìµœê·¼ {days_back}ì¼ í‰ê·  BTI", f"{main_df['bti'].tail(days_back).mean():.2f}")
                        
                        with st.expander("BTI(Brand Trend Index) ê³µì‹ ë³´ê¸°"):
                            st.markdown(r"""
                                $ BTI = \frac{ê²€ìƒ‰ëŸ‰ \ ì§€ìˆ˜}{ê¸°ì¤€ \ í‚¤ì›Œë“œë“¤ì˜ \ ìµœê³  \ ì§€ìˆ˜} \times 100 $
                                <br>
                                *BTIëŠ” 0-100 ì‚¬ì´ì˜ ìƒëŒ€ì  ìˆ˜ì¹˜ì…ë‹ˆë‹¤.*
                                """, unsafe_allow_html=True)

                        st.markdown(f"ìµœê·¼ {days_back}ì¼ BTI ì¶”ì´:")
                        st.dataframe(main_df[['date', 'bti']].tail(days_back).set_index('date'), use_container_width=True)

                        st.subheader("BTI ì§€ìˆ˜ ì¶”ì´ ê·¸ë˜í”„")
                        
                        fig, ax = plt.subplots(figsize=(12, 6))
                        ax.plot(main_df['date'], main_df['bti'], 'b-', linewidth=2, label='BTI ì§€ìˆ˜')
                        
                        main_df['30d_ma'] = main_df['bti'].rolling(window=30).mean()
                        ax.plot(main_df['date'], main_df['30d_ma'], 'r--', linewidth=2, label='30ì¼ ì´ë™í‰ê· ')
                        
                        ax.set_title(f'BTI Index Trend: {keyword} (Naver)')
                        ax.set_xlabel('ë‚ ì§œ')
                        ax.set_ylabel('BTI ì§€ìˆ˜')
                        ax.grid(True, linestyle='--', alpha=0.7)
                        ax.legend()
                        st.pyplot(fig)
                        
                        combined_index = calculate_combined_index(result['spread_coefficient'], main_df)
                        st.subheader("ğŸ”® í†µí•© í™•ì‚° ì ì¬ë ¥ ì§€ìˆ˜")
                        with st.expander("í†µí•© í™•ì‚° ì ì¬ë ¥ ì§€ìˆ˜ ê³µì‹ ë³´ê¸°"):
                            st.markdown(r"""
                                $ í†µí•© \ ì§€ìˆ˜ = \frac{(SC \times 10) + BTI}{2} $
                                <br>
                                *SC(0-10)ì™€ BTI(0-100)ë¥¼ 0-100 ìŠ¤ì¼€ì¼ë¡œ ë§ì¶° í‰ê· ì„ ë‚¸ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.*
                                """, unsafe_allow_html=True)
                        st.metric("í†µí•© í™•ì‚° ì ì¬ë ¥", f"{combined_index:.2f} / 100.00")
                        st.info("YouTube í™•ì‚° ê³„ìˆ˜(SC)ì™€ ë„¤ì´ë²„ BTIë¥¼ í•©ì‚°í•œ ì§€ìˆ˜ë¡œ, í‚¤ì›Œë“œì˜ ë¯¸ë˜ í™•ì‚° ì ì¬ë ¥ì„ ì¶”ì‚°í•©ë‹ˆë‹¤.")

                    except requests.exceptions.HTTPError as err:
                        if err.response.status_code == 401:
                            st.error("ë„¤ì´ë²„ API ì¸ì¦ ì˜¤ë¥˜: Client ID ë˜ëŠ” Client Secretì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        else:
                            st.error(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {err}")
                    except Exception as e:
                        st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            st.warning("API í‚¤ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")

else:
    password_input = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if password_input:
        if password_input == PASSWORD:
            st.session_state.logged_in = True
            st.experimental_rerun()
        else:
            st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")