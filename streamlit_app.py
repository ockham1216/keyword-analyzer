import streamlit as st
import math
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from googleapiclient.discovery import build
from datetime import datetime, timedelta, timezone
from collections import Counter
import re
import time
import requests
import json
import numpy as np
from wordcloud import WordCloud


# Streamlit ì•± ì„¤ì •
st.set_page_config(
Â  Â  page_title="í†µí•© í‚¤ì›Œë“œ ë¶„ì„ê¸°",
Â  Â  layout="wide",
Â  Â  initial_sidebar_state="expanded"
)

# --- í°íŠ¸ ë¡œë“œ (í”„ë¡œì íŠ¸ í´ë”ì— 'font/malgun.ttf'ê°€ ìˆì–´ì•¼ í•¨) ---
try:
Â  Â  font_path = 'font/malgun.ttf'
Â  Â  fm.fontManager.addfont(font_path)
Â  Â  font_name = fm.FontProperties(fname=font_path).get_name()
Â  Â  plt.rcParams['font.family'] = font_name
Â  Â  plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
Â  Â  st.warning(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
Â  Â  st.info("í”„ë¡œì íŠ¸ í´ë”ì— 'font' í´ë”ë¥¼ ë§Œë“¤ê³  'malgun.ttf' íŒŒì¼ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")


# --- 1. YouTube ë¶„ì„ í´ë˜ìŠ¤ ---
class YouTubeSpreadAnalyzer:
Â  Â  def __init__(self, api_key):
Â  Â  Â  Â  self.youtube = build('youtube', 'v3', developerKey=api_key)
Â  Â  Â  Â  self.api_key = api_key
Â  Â  
Â  Â  def search_videos_by_keyword(self, keyword, max_results=100, days_back=30):
Â  Â  Â  Â  """í‚¤ì›Œë“œë¡œ ë¹„ë””ì˜¤ ê²€ìƒ‰ (í˜ì´ì§€ë„¤ì´ì…˜ ë° ë°°ì¹˜ ì²˜ë¦¬)"""
Â  Â  Â  Â  published_after = (datetime.now(timezone.utc) - timedelta(days=days_back)).isoformat()
Â  Â  Â  Â  video_ids = []
Â  Â  Â  Â  next_page_token = None
Â  Â  Â  Â  remaining_results = max_results
Â  Â  Â  Â  
Â  Â  Â  Â  while remaining_results > 0:
Â  Â  Â  Â  Â  Â  current_max = min(50, remaining_results)
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  search_response = self.youtube.search().list(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  q=keyword,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  part="id,snippet",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  maxResults=current_max,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pageToken=next_page_token,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  order="viewCount",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  publishedAfter=published_after,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  type="video",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  relevanceLanguage="ko"
Â  Â  Â  Â  Â  Â  Â  Â  ).execute()
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"YouTube API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  for item in search_response['items']:
Â  Â  Â  Â  Â  Â  Â  Â  if item['id']['kind'] == 'youtube#video':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_ids.append(item['id']['videoId'])
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  next_page_token = search_response.get('nextPageToken')
Â  Â  Â  Â  Â  Â  if not next_page_token:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  remaining_results -= current_max
Â  Â  Â  Â  Â  Â  time.sleep(0.5)
Â  Â  Â  Â  
Â  Â  Â  Â  actual_count = len(video_ids)
Â  Â  Â  Â  if actual_count == 0:
Â  Â  Â  Â  Â  Â  return []
Â  Â  Â  Â  
Â  Â  Â  Â  videos = []
Â  Â  Â  Â  batch_size = 50
Â  Â  Â  Â  for i in range(0, actual_count, batch_size):
Â  Â  Â  Â  Â  Â  batch_ids = video_ids[i:i + batch_size]
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  video_response = self.youtube.videos().list(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  part="statistics,snippet",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  id=",".join(batch_ids)
Â  Â  Â  Â  Â  Â  Â  Â  ).execute()
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"YouTube API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  break

Â  Â  Â  Â  Â  Â  for item in video_response['items']:
Â  Â  Â  Â  Â  Â  Â  Â  view_count = int(item['statistics'].get('viewCount', 0))
Â  Â  Â  Â  Â  Â  Â  Â  like_count = int(item['statistics'].get('likeCount', 0))
Â  Â  Â  Â  Â  Â  Â  Â  comment_count = int(item['statistics'].get('commentCount', 0))
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  videos.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'id': item['id'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'title': item['snippet']['title'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'channelTitle': item['snippet']['channelTitle'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'publishedAt': item['snippet']['publishedAt'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'viewCount': view_count,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'likeCount': like_count,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'commentCount': comment_count
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  time.sleep(0.5)
Â  Â  Â  Â  
Â  Â  Â  Â  return videos
Â  Â  
Â  Â  def get_video_comments(self, video_ids, max_results_per_video=100):
Â  Â      """ê°€ì¥ ì¡°íšŒìˆ˜ê°€ ë†’ì€ ì˜ìƒë“¤ì˜ ëŒ“ê¸€ì„ ê°€ì ¸ì˜´ (ì¿¼í„° ìµœì†Œí™”)"""
Â  Â      all_comments_text = ""
Â  Â      for video_id in video_ids:
Â  Â          try:
Â  Â              comment_response = self.youtube.commentThreads().list(
Â  Â                  part="snippet",
Â  Â                  videoId=video_id,
Â  Â                  maxResults=max_results_per_video,
Â  Â                  order="time" # ìµœì‹  ëŒ“ê¸€ ìˆœ
Â  Â              ).execute()
Â  Â              
Â  Â              for item in comment_response['items']:
Â  Â                  comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
Â  Â                  all_comments_text += comment + " "
Â  Â          except Exception as e:
Â  Â              st.error(f"ëŒ“ê¸€ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
Â  Â              continue
Â  Â              
Â  Â      return all_comments_text
Â  Â  
Â  Â  def calculate_spread_coefficient(self, videos):
Â  Â  Â  Â  if not videos:
Â  Â  Â  Â  Â  Â  return 0.0, 0.0, 0, 0.0
Â  Â  Â  Â  
Â  Â  Â  Â  total_views = 0
Â  Â  Â  Â  total_weighted = 0
Â  Â  Â  Â  video_count = len(videos)
Â  Â  Â  Â  
Â  Â  Â  Â  for video in videos:
Â  Â  Â  Â  Â  Â  view_count = video['viewCount']
Â  Â  Â  Â  Â  Â  like_count = video['likeCount']
Â  Â  Â  Â  Â  Â  comment_count = video['commentCount']
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  engagement = 0
Â  Â  Â  Â  Â  Â  if view_count > 0:
Â  Â  Â  Â  Â  Â  Â  Â  engagement = min(0.45, (like_count + comment_count) / view_count)
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  weighted_views = view_count * (1 + engagement)
Â  Â  Â  Â  Â  Â  total_weighted += weighted_views
Â  Â  Â  Â  Â  Â  total_views += view_count
Â  Â  Â  Â  
Â  Â  Â  Â  avg_views = total_views / video_count
Â  Â  Â  Â  avg_weighted = total_weighted / video_count
Â  Â  Â  Â  
Â  Â  Â  Â  if avg_weighted <= 1000:
Â  Â  Â  Â  Â  Â  spread_coefficient = 0
Â  Â  Â  Â  elif avg_weighted >= 5_000_000:
Â  Â  Â  Â  Â  Â  spread_coefficient = 10.0
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  spread_coefficient = (math.log10(avg_weighted) - 3) * (10 / (math.log10(5_000_000) - 3))
Â  Â  Â  Â  
Â  Â  Â  Â  return spread_coefficient, avg_weighted, total_views, avg_views
Â  Â  
Â  Â  def extract_common_keywords(self, titles, top_n=10):
Â  Â  Â  Â  if not titles:
Â  Â  Â  Â  Â  Â  return []
Â  Â  Â  Â  
Â  Â  Â  Â  stop_words = ["ì˜ìƒ", "ì¶”ì²œ", "ë¹„ë””ì˜¤", "Youtube", "YouTube", "ë³´ê¸°", "ìµœì‹ ", "ì¸ê¸°", "ê¸‰ìƒìŠ¹", "ê³µê°œ", "í’€ì˜ìƒ", "í’€ë²„ì „", "ê³µì‹"]
Â  Â  Â  Â  words = []
Â  Â  Â  Â  for title in titles:
Â  Â  Â  Â  Â  Â  clean_title = re.sub(r'[^\w\s#]', '', title)
Â  Â  Â  Â  Â  Â  hashtags = re.findall(r'#(\w+)', clean_title)
Â  Â  Â  Â  Â  Â  words.extend(hashtags)
Â  Â  Â  Â  Â  Â  korean_words = re.findall(r'[\w#]*[ê°€-í£]{2,}[\w#]*', clean_title)
Â  Â  Â  Â  Â  Â  words.extend(korean_words)
Â  Â  Â  Â  
Â  Â  Â  Â  filtered_words = [word for word in words if word not in stop_words and len(word) > 1]
Â  Â  Â  Â  word_counts = Counter(filtered_words)
Â  Â  Â  Â  
Â  Â  Â  Â  return [f"#{word}" for word, _ in word_counts.most_common(top_n)]
Â  Â  
Â  Â  def analyze_keyword_spread(self, keyword, days_back=30, max_results=100):
Â  Â  Â  Â  videos = self.search_videos_by_keyword(keyword, max_results, days_back)
Â  Â  Â  Â  
Â  Â  Â  Â  if not videos:
Â  Â  Â  Â  Â  Â  return {"error": "ë™ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"}, None
Â  Â  Â  Â  
Â  Â  Â  Â  spread_coeff, avg_weighted, total_views, avg_views = self.calculate_spread_coefficient(videos)
Â  Â  Â  Â  
Â  Â  Â  Â  top_videos = sorted(videos, key=lambda x: x['viewCount'], reverse=True)[:10]
Â  Â  Â  Â  top_titles = [video['title'] for video in top_videos]
Â  Â  Â  Â  common_keywords = self.extract_common_keywords(top_titles)
Â  Â  Â  Â  
Â  Â  Â  Â  result = {
Â  Â  Â  Â  Â  Â  "keyword": keyword,
Â  Â  Â  Â  Â  Â  "total_videos": len(videos),
Â  Â  Â  Â  Â  Â  "total_views": total_views,
Â  Â  Â  Â  Â  Â  "avg_views": avg_views,
Â  Â  Â  Â  Â  Â  "avg_weighted_views": avg_weighted,
Â  Â  Â  Â  Â  Â  "spread_coefficient": spread_coeff,
Â  Â  Â  Â  Â  Â  "top_videos": top_videos,
Â  Â  Â  Â  Â  Â  "common_keywords": common_keywords[:5],
Â  Â  Â  Â  Â  Â  "days_back": days_back
Â  Â  Â  Â  }
Â  Â  Â  Â  
Â  Â  Â  Â  return result, videos

# --- 2. Naver ë¶„ì„ í•¨ìˆ˜ ---
def get_naver_search_index(keywords_dict, start_date, end_date):
Â  Â  url = "https://openapi.naver.com/v1/datalab/search"
Â  Â  headers = {
Â  Â  Â  Â  "X-Naver-Client-Id": st.secrets["NAVER_CLIENT_ID"],
Â  Â  Â  Â  "X-Naver-Client-Secret": st.secrets["NAVER_CLIENT_SECRET"],
Â  Â  Â  Â  "Content-Type": "application/json"
Â  Â  }
Â  Â  
Â  Â  keyword_groups = []
Â  Â  for group_name, keywords in keywords_dict.items():
Â  Â  Â  Â  keyword_groups.append({
Â  Â  Â  Â  Â  Â  "groupName": group_name,
Â  Â  Â  Â  Â  Â  "keywords": keywords
Â  Â  Â  Â  })
Â  Â  
Â  Â  body = {
Â  Â  Â  Â  "startDate": start_date,
Â  Â  Â  Â  "endDate": end_date,
Â  Â  Â  Â  "timeUnit": "date",
Â  Â  Â  Â  "keywordGroups": keyword_groups
Â  Â  }
Â  Â  
Â  Â  res = requests.post(url, json=body, headers=headers)
Â  Â  res.raise_for_status()
Â  Â  return res.json()

def calculate_absolute_index(main_data, ref_data_list):
Â  Â  ref_max = 0
Â  Â  for ref_df in ref_data_list:
Â  Â  Â  Â  ref_max = max(ref_max, ref_df['ratio'].max())
Â  Â  
Â  Â  return (main_data['ratio'] / ref_max) * 100

def calculate_bti(naver_abs_index):
Â  Â  return naver_abs_index

def calculate_combined_index(sc_value, bti_df):
Â  Â  avg_bti = bti_df['bti'].tail(len(bti_df)).mean()
Â  Â  combined_index = (sc_value * 10.0 + avg_bti) / 2.0
Â  Â  return combined_index


# --- ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„± í•¨ìˆ˜ ---
def create_wordcloud(text, font_path):
Â  Â  wordcloud = WordCloud(
Â  Â  Â  Â  font_path=font_path,
Â  Â  Â  Â  background_color="white",
Â  Â  Â  Â  width=800,
Â  Â  Â  Â  height=400,
Â  Â  Â  Â  max_words=50
Â  Â  ).generate(text)
Â  Â  
Â  Â  fig, ax = plt.subplots(figsize=(10, 5))
Â  Â  ax.imshow(wordcloud, interpolation="bilinear")
Â  Â  ax.axis("off")
Â  Â  st.pyplot(fig)

# --- ì‚¬ìš©ì ì¸ì¦ ---
PASSWORD = st.secrets["APP_PASSWORD"]
if 'logged_in' not in st.session_state:
Â  Â  st.session_state.logged_in = False

if st.session_state.logged_in:
Â  Â  # --- Streamlit UI ë° ë©”ì¸ ë¡œì§ ---
Â  Â  st.title("í†µí•© í‚¤ì›Œë“œ í™•ì‚° ë¶„ì„ê¸°")
Â  Â  st.markdown("ìœ íŠœë¸Œì™€ ë„¤ì´ë²„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ í‚¤ì›Œë“œì˜ í™•ì‚°ë ¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

Â  Â  # ì‚¬ì´ë“œë°”
Â  Â  with st.sidebar:
Â  Â  Â  Â  st.header("ğŸ”‘ í‚¤ì›Œë“œ ë° ì„¤ì •")
Â  Â  Â  Â  keyword = st.text_input("ë¶„ì„í•  í‚¤ì›Œë“œ", placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”")
Â  Â  Â  Â  days_back = st.slider("ë¶„ì„ ê¸°ê°„ (ì¼)", 3, 365, 30)
Â  Â  Â  Â  max_results = st.slider("YouTube ë¶„ì„ ë™ì˜ìƒ ìˆ˜", 10, 200, 100)
Â  Â  Â  Â  
Â  Â  Â  Â  st.subheader("ë„¤ì´ë²„ BTI ê¸°ì¤€ í‚¤ì›Œë“œ")
Â  Â  Â  Â  ref_keywords_str = st.text_input("ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ì…ë ¥", "ë‰´ìŠ¤,ë‚ ì”¨")
Â  Â  Â  Â  reference_keywords = [kw.strip() for kw in ref_keywords_str.split(',') if kw.strip()]
Â  Â  Â  Â  
Â  Â  Â  Â  run_button = st.button("ğŸš€ ë¶„ì„ ì‹œì‘")

Â  Â  if run_button and keyword:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  tab1, tab2 = st.tabs(["ğŸ“Š ìœ íŠœë¸Œ í™•ì‚° ë¶„ì„", "ğŸ“ˆ ë„¤ì´ë²„ BTI ë¶„ì„"])

Â  Â  Â  Â  Â  Â  with tab1:
Â  Â  Â  Â  Â  Â  Â  Â  st.header("ğŸ“Š ìœ íŠœë¸Œ í™•ì‚° ë¶„ì„ ê²°ê³¼")
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("YouTube ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  youtube_analyzer = YouTubeSpreadAnalyzer(st.secrets["YOUTUBE_API_KEY"])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result, videos = youtube_analyzer.analyze_keyword_spread(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keyword,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  days_back=days_back,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_results=max_results
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  if "error" in result:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"ì˜¤ë¥˜ ë°œìƒ: {result['error']}")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader(f"'{result['keyword']}' í‚¤ì›Œë“œ í™•ì‚° ë¶„ì„ (ìµœê·¼ {result['days_back']}ì¼ ê¸°ì¤€)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("í™•ì‚° ê³„ìˆ˜ (SC)", f"{result['spread_coefficient']:.2f} / 10.00")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("í™•ì‚° ê³„ìˆ˜(SC) ê³µì‹ ë³´ê¸°"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(r"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  $ SC_{coeff} = \frac{log_{10}(WAV) - 3}{log_{10}(5,000,000) - 3} \times 10 $
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <br>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **WAV**: ê°€ì¤‘ í‰ê·  ì¡°íšŒìˆ˜ (Weighted Average Views) = ì¼ë°˜ ì¡°íšŒìˆ˜ Ã— (1 + ì°¸ì—¬ë„)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <br>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **ì°¸ì—¬ë„**: (ì¢‹ì•„ìš” + ëŒ“ê¸€) / ì¡°íšŒìˆ˜
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """, unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"**ì´ ì¡°íšŒìˆ˜**: {result['total_views']:,}íšŒ | **í‰ê·  ì¡°íšŒìˆ˜**: {result['avg_views']:,.1f}íšŒ | **í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜**: {result['avg_weighted_views']:,.1f}íšŒ")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ìƒˆë¡œìš´ ì§€í‘œ: í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜ / í‰ê·  ì¡°íšŒìˆ˜ ë¹„ìœ¨
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if result['avg_views'] > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  engagement_ratio = result['avg_weighted_views'] / result['avg_views']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("ì°¸ì—¬ë„ ì˜í–¥ë ¥ (ê°€ì¤‘/ì¼ë°˜ ì¡°íšŒìˆ˜)", f"{engagement_ratio:.2f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sc = result['spread_coefficient']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sc_guide = ""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if sc < 2.0: sc_guide = "ë¯¸ë¯¸í•œ ì˜í–¥"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif sc < 4.0: sc_guide = "ì£¼ëª© ìš”ë§"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif sc < 6.0: sc_guide = "ìœ ì˜ë¯¸í•œ ì˜í–¥"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif sc < 8.0: sc_guide = "ì‹¬ê°í•œ ì˜í–¥"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif sc < 10.0: sc_guide = "ìœ„ê¸° ìˆ˜ì¤€"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else: sc_guide = "ìµœê³  ìœ„ê¸° ìˆ˜ì¤€"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**í•´ì„**: {sc_guide}")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**ì¶”ì²œ í•´ì‹œíƒœê·¸**: `{'`, `'.join(result['common_keywords'])}`")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ìƒìœ„ ë™ì˜ìƒ ëª©ë¡")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top_videos_df = pd.DataFrame(result['top_videos'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top_videos_df['publishedAt'] = pd.to_datetime(top_videos_df['publishedAt']).dt.strftime('%Y-%m-%d')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top_videos_df = top_videos_df[['channelTitle', 'title', 'viewCount', 'likeCount', 'commentCount', 'publishedAt']]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(top_videos_df, use_container_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ì£¼ìš” ì‹œê°í™”")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(16, 6))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  metrics = ['í‰ê·  ì¡°íšŒìˆ˜', 'í‰ê·  ê°€ì¤‘ ì¡°íšŒìˆ˜']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  values = [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result['avg_views'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result['avg_weighted_views']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colors = ['green', 'orange']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.bar(metrics, values, color=colors)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.set_title('í™•ì‚° ì§€í‘œ ë¹„êµ')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.set_ylabel('ê°’')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.ticklabel_format(style='plain', axis='y')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  plt.tight_layout()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(fig)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ì›Œë“œí´ë¼ìš°ë“œ
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_titles_text = " ".join([video['title'] for video in videos])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ğŸ’¬ ì˜ìƒ ì œëª© ì›Œë“œ í´ë¼ìš°ë“œ")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if all_titles_text:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  create_wordcloud(all_titles_text, font_path)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("ì›Œë“œ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ì œëª©ì´ ì—†ìŠµë‹ˆë‹¤.")


Â  Â  Â  Â  Â  Â  with tab2:
Â  Â  Â  Â  Â  Â  Â  Â  st.header("ğŸ“ˆ ë„¤ì´ë²„ BTI ë¶„ì„ ê²°ê³¼")
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Naver ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  end_date = datetime.now().strftime("%Y-%m-%d")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  start_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keywords_dict = {"main": [keyword]}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for i, ref_kw in enumerate(reference_keywords):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  keywords_dict[f"ref_{i}"] = [ref_kw]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  naver_raw = get_naver_search_index(keywords_dict, start_date, end_date)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results = {}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for res in naver_raw['results']:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  group_name = res['title']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df = pd.DataFrame(res['data'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df['date'] = pd.to_datetime(df['period'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results[group_name] = df
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  main_df = results['main']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ref_dfs = [df for key, df in results.items() if key.startswith('ref_')]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  main_df['abs_index'] = calculate_absolute_index(main_df, ref_dfs)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  main_df['bti'] = calculate_bti(main_df['abs_index'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader(f"'{keyword}' í‚¤ì›Œë“œ BTI ë¶„ì„ (ìµœê·¼ {days_back}ì¼ ê¸°ì¤€)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric(f"ìµœê·¼ {days_back}ì¼ í‰ê·  BTI", f"{main_df['bti'].tail(days_back).mean():.2f}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("BTI(Brand Trend Index) ê³µì‹ ë³´ê¸°"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(r"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  $ BTI = \frac{ê²€ìƒ‰ëŸ‰ \ ì§€ìˆ˜}{ê¸°ì¤€ \ í‚¤ì›Œë“œë“¤ì˜ \ ìµœê³  \ ì§€ìˆ˜} \times 100 $
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <br>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  *BTIëŠ” 0-100 ì‚¬ì´ì˜ ìƒëŒ€ì  ìˆ˜ì¹˜ì…ë‹ˆë‹¤.*
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """, unsafe_allow_html=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"ìµœê·¼ {days_back}ì¼ BTI ì¶”ì´:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(main_df[['date', 'bti']].tail(days_back).set_index('date'), use_container_width=True)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("BTI ì§€ìˆ˜ ì¶”ì´ ê·¸ë˜í”„")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig, ax = plt.subplots(figsize=(12, 6))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.plot(main_df['date'], main_df['bti'], 'b-', linewidth=2, label='BTI ì§€ìˆ˜')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  main_df['30d_ma'] = main_df['bti'].rolling(window=30).mean()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.plot(main_df['date'], main_df['30d_ma'], 'r--', linewidth=2, label='30ì¼ ì´ë™í‰ê· ')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.set_title(f'BTI Index Trend: {keyword} (Naver)')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.set_xlabel('ë‚ ì§œ')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.set_ylabel('BTI ì§€ìˆ˜')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.grid(True, linestyle='--', alpha=0.7)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax.legend()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(fig)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  combined_index = calculate_combined_index(result['spread_coefficient'], main_df)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ğŸ”® í†µí•© í™•ì‚° ì ì¬ë ¥ ì§€ìˆ˜")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("í†µí•© í™•ì‚° ì ì¬ë ¥ ì§€ìˆ˜ ê³µì‹ ë³´ê¸°"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(r"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  $ í†µí•© \ ì§€ìˆ˜ = \frac{(SC \times 10) + BTI}{2} $
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <br>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  *SC(0-10)ì™€ BTI(0-100)ë¥¼ 0-100 ìŠ¤ì¼€ì¼ë¡œ ë§ì¶° í‰ê· ì„ ë‚¸ ìˆ˜ì¹˜ì…ë‹ˆë‹¤.*
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """, unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("í†µí•© í™•ì‚° ì ì¬ë ¥", f"{combined_index:.2f} / 100.00")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("YouTube í™•ì‚° ê³„ìˆ˜(SC)ì™€ ë„¤ì´ë²„ BTIë¥¼ í•©ì‚°í•œ ì§€ìˆ˜ë¡œ, í‚¤ì›Œë“œì˜ ë¯¸ë˜ í™•ì‚° ì ì¬ë ¥ì„ ì¶”ì‚°í•©ë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except requests.exceptions.HTTPError as err:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if err.response.status_code == 401:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error("ë„¤ì´ë²„ API ì¸ì¦ ì˜¤ë¥˜: Client ID ë˜ëŠ” Client Secretì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"ë„¤ì´ë²„ API í˜¸ì¶œ ì˜¤ë¥˜: {err}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"ë¶„ì„ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
Â  Â  Â  Â  Â  Â  st.warning("API í‚¤ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í–ˆëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")

else:
Â  Â  # --- ë¡œê·¸ì¸ ì „, ë¹„ë°€ë²ˆí˜¸ ì…ë ¥ UI ---
Â  Â  st.header("ğŸ”‘ í†µí•© í‚¤ì›Œë“œ ë¶„ì„ê¸° ë¡œê·¸ì¸")
Â  Â  password_input = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
Â  Â  if password_input:
Â  Â  Â  Â  if password_input == PASSWORD:
Â  Â  Â  Â  Â  Â  st.session_state.logged_in = True
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")